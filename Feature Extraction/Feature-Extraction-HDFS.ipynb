{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Squential Event Feature Extraction\r\n",
    "\r\n",
    "### Hadoop Distributed File System\r\n",
    "The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware.\r\n",
    "\r\n",
    "Log Structure | Description\r\n",
    "--- | ---\r\n",
    "yy/MM/dd | date\r\n",
    "hh/mm/ss | time\r\n",
    "_ _ _ _ | a 2-4 digit code (not sure exactly what it represents)\r\n",
    "INFO/WARN/DEBUG/ERROR/etc. | type of the logging event and priority\r\n",
    "dfs.____:_____ | general source of logging event\r\n",
    "\\_____\\<blk id\\>_____ | log message\r\n",
    "\r\n",
    "Example:\r\n",
    "- `081109 203807 222 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6952295868487656571 terminating`\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# parsing and wrangling\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import regex as re\r\n",
    "\r\n",
    "# misc\r\n",
    "from datetime import datetime as dt\r\n",
    "import itertools"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HDFS Feature Engineering\n",
    "\n",
    "In the future, incorporating the time between sequential log events will be considered as it may be a useful feature for improving anomaly detection."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## import raw log data\r\n",
    "\r\n",
    "# full set\r\n",
    "raw = pd.read_csv('All-Data/HDFS/Raw/HDFS.log', header=None, sep='\\n')[0]\r\n",
    "\r\n",
    "# sample set\r\n",
    "#raw = pd.read_csv('../All-Data/HDFS/Raw/HDFS_2k.log', header=None, sep='\\n')[0]\r\n",
    "\r\n",
    "raw.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    081109 203615 148 INFO dfs.DataNode$PacketResp...\n",
       "1    081109 203807 222 INFO dfs.DataNode$PacketResp...\n",
       "2    081109 204005 35 INFO dfs.FSNamesystem: BLOCK*...\n",
       "3    081109 204015 308 INFO dfs.DataNode$PacketResp...\n",
       "4    081109 204106 329 INFO dfs.DataNode$PacketResp...\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "## import anomaly labels\r\n",
    "\r\n",
    "labels = pd.read_csv('../All-Data/HDFS/Raw/anomaly_label.csv')\r\n",
    "labels.Label = [1 if x == \"Anomaly\" else 0 for x in labels.Label]\r\n",
    "\r\n",
    "length = len(labels)\r\n",
    "anomalies = len(labels[labels.Label == 1])\r\n",
    "\r\n",
    "print('Length labels: ', length)\r\n",
    "print('Anomalies: ', anomalies)\r\n",
    "print('% Anomalous: ', round(anomalies/length*100, 2),'%')\r\n",
    "\r\n",
    "labels.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length labels:  575061\n",
      "Anomalies:  16838\n",
      "% Anomalous:  2.93 %\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    BlockId  Label\n",
       "0  blk_-1608999687919862906      0\n",
       "1   blk_7503483334202473044      0\n",
       "2  blk_-3544583377289625738      1\n",
       "3  blk_-9073992586687739851      0\n",
       "4   blk_7854771516489510256      0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "## parse for block IDs\r\n",
    "\r\n",
    "# extract ids from log in the correct event order\r\n",
    "blocks_in_order = raw.str.findall(r'(blk_.[\\d]*)')\r\n",
    "unlist_vectorized = np.vectorize(lambda x: x[0])\r\n",
    "blocks_in_order = pd.Series(unlist_vectorized(blocks_in_order))\r\n",
    "\r\n",
    "# label the extracted block ids via a conversion dictionary\r\n",
    "binarizer = dict(zip(labels.BlockId, labels.Label))\r\n",
    "binarizer_vectorized = np.vectorize(lambda x: binarizer[x])\r\n",
    "blocks_binarized = pd.Series(binarizer_vectorized(blocks_in_order))\r\n",
    "\r\n",
    "labeled_blks = pd.DataFrame({'blkID':blocks_in_order, 'anomaly':blocks_binarized})\r\n",
    "# checkpoint\r\n",
    "#labeled_blks.to_feather('Data/labeled_blks.feather')\r\n",
    "\r\n",
    "print('Unique blocks in Log File: ', len(blocks_in_order.unique()))\r\n",
    "print('Anomalous blocks in the sample Log File: ', binarizer_vectorized(blocks_in_order.unique()).sum()) \r\n",
    "\r\n",
    "labeled_blks.head()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique blocks in Log File:  1994\n",
      "Anomalous blocks in the sample Log File:  68\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                      blkID  anomaly\n",
       "0     blk_38865049064139660        0\n",
       "1  blk_-6952295868487656571        0\n",
       "2   blk_7128370237687728475        0\n",
       "3   blk_8229193803249955061        0\n",
       "4  blk_-6670958622368987959        0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blkID</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_38865049064139660</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_-6952295868487656571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_7128370237687728475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_8229193803249955061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_-6670958622368987959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "## extract raw messages\r\n",
    "\r\n",
    "full_msg = raw.str.extract(r'((?<=:\\s).*)')[0]\r\n",
    "full_msg.head()\r\n",
    "\r\n",
    "# checkpoint\r\n",
    "#full_msg.to_csv('Data/full_msg.csv', index=None, header=None)\r\n",
    "#full_msg = pd.read_csv('Data/full_msg.csv', index_col=False, header=None, squeeze=True)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    PacketResponder 1 for block blk_38865049064139...\n",
       "1    PacketResponder 0 for block blk_-6952295868487...\n",
       "2    BLOCK* NameSystem.addStoredBlock: blockMap upd...\n",
       "3    PacketResponder 2 for block blk_82291938032499...\n",
       "4    PacketResponder 2 for block blk_-6670958622368...\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "## remove all unique event identifiers (block ids) to obtain a general message structure\r\n",
    "\r\n",
    "# split log into chunks for RAM friendly processing\r\n",
    "n_chunks = 200\r\n",
    "chunked_msgs = np.array_split(full_msg, n_chunks)\r\n",
    "\r\n",
    "# vectorized function to join key words from list to str\r\n",
    "toSentence = np.vectorize(lambda x: \" \".join(x))\r\n",
    "\r\n",
    "# for storing generalized sentences\r\n",
    "str_msgs = pd.Series(dtype='object')\r\n",
    "\r\n",
    "for chunk in range(n_chunks):\r\n",
    "    # extract and join key words without IDs\r\n",
    "    sentences = pd.Series(toSentence(chunked_msgs[chunk].str.findall(r'([A-Za-z]+)')))\r\n",
    "    str_msgs = pd.concat([str_msgs, sentences])\r\n",
    "    # save RAM\r\n",
    "    del sentences\r\n",
    "\r\n",
    "print('Messages extracted: ', len(str_msgs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Messages extracted:  2000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "## convert general message structure to numeric\r\n",
    "\r\n",
    "coded_msgs = pd.Categorical(str_msgs).codes \r\n",
    "\r\n",
    "print('Unique Message Types: ', len(pd.Series(coded_msgs).unique()))\r\n",
    "\r\n",
    "coded_msgs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique Message Types:  19\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([12, 12,  0, ..., 15, 12, 15], dtype=int8)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "## create final sequetial feature\r\n",
    "\r\n",
    "# earlier checkpoint\r\n",
    "#labeled_blks = pd.read_feather('Data/labeled_blks.feather')\r\n",
    "\r\n",
    "# block ID and event codes frame\r\n",
    "blk_events = pd.DataFrame({'blk_ID': labeled_blks.blkID, 'msg_code':coded_msgs})\r\n",
    "\r\n",
    "# groupby by block ID to create event sequences \r\n",
    "blk_event_sequences = blk_events.groupby('blk_ID')['msg_code'].apply(list).reset_index(name='sequence')\r\n",
    "\r\n",
    "# add anomaly labels using a vectorized labelling dictionary\r\n",
    "blk_key = dict(labels.values)\r\n",
    "vectorized_anom_labeler = np.vectorize(lambda x: blk_key[x])\r\n",
    "blk_event_sequences['anomaly'] = pd.Series(vectorized_anom_labeler(blk_event_sequences.blk_ID))\r\n",
    "\r\n",
    "# export the feature\r\n",
    "blk_event_sequences.to_csv('../Data/blk_event_sequences.csv', index = False, header = True)\r\n",
    "\r\n",
    "blk_event_sequences.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     blk_ID sequence  anomaly\n",
       "0  blk_-1030832046197982436      [4]        0\n",
       "1  blk_-1046472716157313227     [10]        0\n",
       "2  blk_-1049340855430710153     [12]        0\n",
       "3  blk_-1055254430948037872      [0]        0\n",
       "4  blk_-1067234447809438340     [12]        0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blk_ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1030832046197982436</td>\n",
       "      <td>[4]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_-1046472716157313227</td>\n",
       "      <td>[10]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-1049340855430710153</td>\n",
       "      <td>[12]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-1055254430948037872</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_-1067234447809438340</td>\n",
       "      <td>[12]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "## perform a quick check\r\n",
    "\r\n",
    "labels = pd.read_csv('../All-Data/HDFS/Raw/anomaly_label.csv')\r\n",
    "labels.Label = [1 if x == \"Anomaly\" else 0 for x in labels.Label]\r\n",
    "\r\n",
    "# how many block IDs in the final dataset?\r\n",
    "target_row_count = len(labels)\r\n",
    "final_row_count = len(blk_event_sequences)\r\n",
    "\r\n",
    "# how many anomalies in the final dataset?\r\n",
    "original_anomaly_count = len(labels[labels.Label==1])\r\n",
    "final_anomaly_count = len(blk_event_sequences[blk_event_sequences.anomaly==1])\r\n",
    "\r\n",
    "\r\n",
    "# if the numbers don't match two things may be happening:\r\n",
    "# 1) something went wrong earlier\r\n",
    "# 2) you're using the sample dataset\r\n",
    "\r\n",
    "print(f'Total blocks present {final_row_count} of {target_row_count}')\r\n",
    "print(f'Total anomalies present {final_anomaly_count} of {original_anomaly_count}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total blocks present 1994 of 575061\n",
      "Total anomalies present 68 of 16838\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}