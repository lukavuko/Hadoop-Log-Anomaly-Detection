{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "identical-declaration",
   "metadata": {},
   "source": [
    "# Sequntial Event Feature Extraction Notebook\n",
    "\n",
    "### Log Structure\n",
    "The HDFS log is built the following way:\n",
    "- %d{yy/MM/dd HH:mm:ss}: The date and time\n",
    "- %???: Unknown 2-4 digit code\n",
    "- %p: The priority of the logging event (INFO, WARN, DEBUG, ERROR, etc.)\n",
    "- %c: Category of logging event (class name)\n",
    "- %m: Log message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-adaptation",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "novel-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "# misc\n",
    "from datetime import datetime as dt\n",
    "import itertools\n",
    "\n",
    "#import cupy as cp\n",
    "#import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "built-chair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    081109 203518 143 INFO dfs.DataNode$DataXceive...\n",
       "1    081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...\n",
       "2    081109 203519 143 INFO dfs.DataNode$DataXceive...\n",
       "3    081109 203519 145 INFO dfs.DataNode$DataXceive...\n",
       "4    081109 203519 145 INFO dfs.DataNode$PacketResp...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import Raw Log Data\n",
    "raw = pd.read_csv('Data/HDFS.log', header=None, sep='\\n')[0]\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-recording",
   "metadata": {},
   "source": [
    "## Code Parsing Section\n",
    "\n",
    "Here we'll parse the log file for block IDs which we will use this to aggregate message sequences for the corresponding block ID. \n",
    "\n",
    "Note the checkpoint cells. Due to memory limitations, checkpoints were created where the kernel could be restarted to free RAM for the next processing cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-studio",
   "metadata": {},
   "source": [
    "### Get BLock IDs and their Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "governmental-macro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len labels:  575061\n",
      "Anomaly count:  16838\n",
      "% Anomalous:  2.93 %\n"
     ]
    }
   ],
   "source": [
    "## Import Anomaly Labels\n",
    "labels = pd.read_csv('Data/anomaly_label.csv')\n",
    "labels.iloc[:,1][labels.Label == 'Anomaly'] = 1\n",
    "labels.iloc[:,1][labels.Label == 'Normal'] = 0\n",
    "\n",
    "length = len(labels)\n",
    "anomalies = len(labels[labels.Label==1])\n",
    "print('Len labels: ', length)\n",
    "print('Anomaly count: ', anomalies)\n",
    "print('% Anomalous: ', round(anomalies/length*100, 2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-spanish",
   "metadata": {},
   "source": [
    "### Parse Log for BlockIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suburban-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Blocks from Log\n",
    "blocks_in_order = raw.str.findall(r'(blk_.[\\d]*)')\n",
    "unlist_vectorized = np.vectorize(lambda x: x[0])\n",
    "blocks_in_order = pd.Series(unlist_vectorized(blocks_in_order))\n",
    "\n",
    "\n",
    "## Label the extracted block ids via conversion dictionary\n",
    "binarizer = dict(zip(labels.BlockId, labels.Label))\n",
    "binarizer_vectorized = np.vectorize(lambda x: binarizer[x])\n",
    "blocks_binarized = pd.Series(binarizer_vectorized(blocks_in_order))\n",
    "\n",
    "## Create export checkpoint\n",
    "labeled_blks = pd.DataFrame({'blkID':blocks_in_order, 'anomaly':blocks_binarized})\n",
    "labeled_blks.head()\n",
    "labeled_blks.to_feather('Data/labeled_blks.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bored-manchester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique blocks in Log File:  575061\n",
      "Anomalies in the Log File:  16838\n"
     ]
    }
   ],
   "source": [
    "print('Unique blocks in Log File: ', len(blocks_in_order.unique()))\n",
    "print('Anomalies in the Log File: ', binarizer_vectorized(blocks_in_order.unique()).sum()) # anomalous **blocks** in the log file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-northern",
   "metadata": {},
   "source": [
    "*The number of unique block IDs and anomaly counts in both the parsed log file and labeled file are equal from the printed outputs. This confirms that the parsing was done correctly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-sucking",
   "metadata": {},
   "source": [
    "### Parse for Full Message Content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neutral-literature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Receiving block blk_-1608999687919862906 src: ...\n",
       "1    BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...\n",
       "2    Receiving block blk_-1608999687919862906 src: ...\n",
       "3    Receiving block blk_-1608999687919862906 src: ...\n",
       "4    PacketResponder 1 for block blk_-1608999687919...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Raw Messages\n",
    "full_msg = raw.str.extract(r'((?<=:\\s).*)')[0]\n",
    "full_msg.to_csv('Data/full_msg.csv', index=None, header=None)\n",
    "\n",
    "# CHECKPOINT\n",
    "full_msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-montana",
   "metadata": {},
   "source": [
    "### Next we remove all unique event identifiers to get general message structures.\n",
    "\n",
    "We convert them to numeric and label each log event this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regional-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Raw message checkpoint\n",
    "full_msg = pd.read_csv('Data/full_msg.csv', index_col=False, header=None, squeeze=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "attempted-plymouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages extracted:  11175629\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks for easier processing\n",
    "chunks = 200\n",
    "chunked_msgs = np.array_split(full_msg, chunks)\n",
    "\n",
    "# Function to join key words\n",
    "toSentence = np.vectorize(lambda x: \" \".join(x))\n",
    "\n",
    "str_msgs = pd.Series(dtype='object')\n",
    "for chunk in range(chunks):\n",
    "    # Extract and join key words without IDs\n",
    "    sentences = pd.Series(toSentence(chunked_msgs[chunk].str.findall(r'([A-Za-z]+)')))\n",
    "    str_msgs = pd.concat([str_msgs, sentences])\n",
    "    # Save that juicy RAM\n",
    "    del sentences\n",
    "\n",
    "print('Messages extracted: ', len(str_msgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-upgrade",
   "metadata": {},
   "source": [
    "### Now lets convert general message structures to numerical codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "usual-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Message Types:  75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([55,  4, 55, ..., 62, 62, 62], dtype=int8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_msgs = pd.Categorical(str_msgs).codes \n",
    "\n",
    "print('Unique Message Types: ', len(pd.Series(coded_msgs).unique()))\n",
    "\n",
    "coded_msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-literacy",
   "metadata": {},
   "source": [
    "### Create a Feature for Block ID Message Type Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "hispanic-salmon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blk_ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1000002529962039464</td>\n",
       "      <td>[55, 55, 55, 16, 51, 53, 51, 53, 3, 3, 3, 51, 53]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_-100000266894974466</td>\n",
       "      <td>[20, 55, 55, 55, 3, 3, 3, 51, 53, 51, 53, 51, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-1000007292892887521</td>\n",
       "      <td>[55, 55, 16, 55, 51, 53, 51, 53, 51, 53, 3, 3, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-1000014584150379967</td>\n",
       "      <td>[55, 20, 55, 55, 3, 3, 3, 51, 53, 51, 53, 51, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_-1000028658773048709</td>\n",
       "      <td>[55, 55, 55, 20, 51, 53, 51, 53, 51, 53, 3, 3,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     blk_ID  \\\n",
       "0  blk_-1000002529962039464   \n",
       "1   blk_-100000266894974466   \n",
       "2  blk_-1000007292892887521   \n",
       "3  blk_-1000014584150379967   \n",
       "4  blk_-1000028658773048709   \n",
       "\n",
       "                                            sequence  anomaly  \n",
       "0  [55, 55, 55, 16, 51, 53, 51, 53, 3, 3, 3, 51, 53]        0  \n",
       "1  [20, 55, 55, 55, 3, 3, 3, 51, 53, 51, 53, 51, ...        0  \n",
       "2  [55, 55, 16, 55, 51, 53, 51, 53, 51, 53, 3, 3, 3]        0  \n",
       "3  [55, 20, 55, 55, 3, 3, 3, 51, 53, 51, 53, 51, ...        0  \n",
       "4  [55, 55, 55, 20, 51, 53, 51, 53, 51, 53, 3, 3,...        0  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import anomaly labels\n",
    "labeled_blks = pd.read_feather('Data/labeled_blks.feather')\n",
    "\n",
    "# Log dataframe of blocks and event category in order\n",
    "blk_events = pd.DataFrame({'blk_ID': labeled_blks.blkID, 'msg_code':coded_msgs})\n",
    "\n",
    "# Groupby by block to create event sequences \n",
    "blk_event_sequences = blk_events.groupby('blk_ID')['msg_code'].apply(list).reset_index(name='sequence')\n",
    "\n",
    "# Assign anomaly labels using a dictionary\n",
    "blk_key = dict(labels.values)\n",
    "vectorized_anom_labeler = np.vectorize(lambda x: blk_key[x])\n",
    "\n",
    "blk_event_sequences['anomaly'] = pd.Series(vectorized_anom_labeler(blk_event_sequences.blk_ID))\n",
    "\n",
    "# Export the golden feature\n",
    "blk_event_sequences.to_csv('Data/blk_event_sequences.csv', index = False, header = True)\n",
    "\n",
    "blk_event_sequences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "incorrect-screen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks present 575061 of 575061\n",
      "Total anomalies present 16838 of 16838\n"
     ]
    }
   ],
   "source": [
    "# Quick Check\n",
    "labels = pd.read_csv('Data/anomaly_label.csv')\n",
    "labels.iloc[:,1][labels.Label == 'Anomaly'] = 1\n",
    "labels.iloc[:,1][labels.Label == 'Normal'] = 0\n",
    "\n",
    "target_rows = len(labels)\n",
    "final_rows = len(blk_event_sequences)\n",
    "\n",
    "original_anomalies = len(labels[labels.Label==1])\n",
    "final_anomalies = len(blk_event_sequences[blk_event_sequences.anomaly==1])\n",
    "\n",
    "print(f'Total blocks present {final_rows} of {target_rows}')\n",
    "print(f'Total anomalies present {final_anomalies} of {original_anomalies}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
