{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Pad Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Percent Anomalous: 2.93 %\n"
     ]
    }
   ],
   "source": [
    "## Import Data\n",
    "\n",
    "# sequence column lists are in string format so use literal eval to reconvert\n",
    "blk_msg_sequence = pd.read_csv('Data/blk_event_sequences.csv', header=0, index_col=None, converters={\"sequence\": literal_eval})\n",
    "\n",
    "# Pad sequences (Predictor)\n",
    "padded_seq = pad_sequences(blk_msg_sequence.sequence, padding='post')\n",
    "\n",
    "# Anomaly Labels (Response)\n",
    "labels = blk_msg_sequence['anomaly']\n",
    "\n",
    "#print(f'Sequence Example: {padded_seq[19]}')\n",
    "#print(f'Label Example: {labels[19]}')\n",
    "print(f'Percent Anomalous: {round(sum(labels)/len(labels)*100, 2)} %')\n",
    "\n",
    "# all_events = blk_msg_sequence.sequence.apply(pd.Series).stack().reset_index(drop = True)\n",
    "# Unique Words (75)\n",
    "# len(all_events.unique())\n",
    "# > 75 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train: 0.7 (402542)\nVal: 0.1 (57506)\nTest: 0.2(115013)\nTotal: 1 (575061)\n"
     ]
    }
   ],
   "source": [
    "# Split Train 0.70\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.70)\n",
    "for train_index, rest_index in sss.split(padded_seq, labels):\n",
    "    x_train, x_other = padded_seq[train_index], padded_seq[rest_index]\n",
    "    y_train, y_other = labels[train_index], labels[rest_index]\n",
    "\n",
    "\n",
    "# Split Val 0.10 / Test 0.20\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_other, y_other, test_size=2/3)\n",
    "\n",
    "\n",
    "# Standardize to arrays\n",
    "y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)\n",
    "\n",
    "\n",
    "# Check proportions\n",
    "trn, val, tst = len(y_train), len(y_val), len(y_test)\n",
    "tot = sum([trn, val, tst])\n",
    "print(f'Train: {round(trn/tot, 2)} ({trn})',\n",
    "      f'Val: {round(val/tot, 2)} ({val})',\n",
    "      f'Test: {round(tst/tot, 2)}({tst})',\n",
    "      f'Total: {1} ({tot})', sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Dimensions\n",
      "\n",
      "Sequence matrix size: torch.Size([1000, 298])\n",
      "Target vector size: torch.Size([1000])\n",
      "\n",
      "Sequence matrix size: torch.Size([1000, 298])\n",
      "Target vector size: torch.Size([1000])\n",
      "\n",
      "Sequence matrix size: torch.Size([1000, 298])\n",
      "Target vector size: torch.Size([1000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "\n",
    "\n",
    "################################\n",
    "## Create Tensor Datasets \n",
    "################################\n",
    "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "val_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n",
    "test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "\n",
    "\n",
    "################################\n",
    "## Dataset Iterators:\n",
    "################################\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=BATCH_SIZE, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, pin_memory=True, drop_last=True)\n",
    "\n",
    "print('Training Dimensions\\n')\n",
    "for data_cat in [train_loader, val_loader, test_loader]:\n",
    "    for batch in data_cat:\n",
    "        print(f'Sequence matrix size: {batch[0].size()}')\n",
    "        print(f'Target vector size: {batch[1].size()}\\n')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch LSTM Model\n",
    "And associated functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## ARCHITECTURAL PARAMETERS\n",
    "################################\n",
    "VOCAB_SIZE = 75+1\n",
    "OUTPUT_SIZE = 1\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "CLIP = 5\n",
    "\n",
    "################################\n",
    "## MODEL\n",
    "################################\n",
    "class Anomaly_Detector(nn.Module):\n",
    "    def __init__(self,  vocab_size, output_size, embedding_dim, hidden_dim, batch_size, n_layers, bidirectinoal=True, drop_prob=0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Params\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bidir = 2 if bidirectinoal==True else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            batch_first=True,\n",
    "                            num_layers=n_layers, \n",
    "                            dropout=drop_prob,  \n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*self.bidir, output_size) # multiply hidden by 2 if using bidirectinoality\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, sequences): \n",
    "        sequences = sequences.long().to(DEVICE) # [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(sequences)) # [batch_size, seq_length, embedded_dim]\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded) \n",
    "        # lstm_out = [batch_size, seq_length, hidden_dim * num_directions]\n",
    "        # hidden = [num layers * num directions,  batch size, hidden_dim]\n",
    "        # cell = [num layers * num directions, batch size, hidden_dim]\n",
    "        \n",
    "        if self.bidir==2:\n",
    "            # Concat the final forward and barckward hidden layers + apply dropout if specified\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        elif self.bidir==1: \n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        out = self.activation(self.fc(hidden).reshape(-1))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "################################\n",
    "## DEFINE ACCURACY FUNCTION\n",
    "################################\n",
    "def compute_binary_accuracy(model, data_loader, device, print_output=False):\n",
    "    model.eval()\n",
    "    all_preds = torch.empty(0).cuda()\n",
    "    all_labs = torch.empty(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inpt, labs in data_loader:\n",
    "\n",
    "            inpt, labs = inpt.cuda(), labs.cuda()\n",
    "            all_labs = torch.cat((all_labs, labs), 0).reshape(-1)\n",
    "\n",
    "            logits = model(inpt)\n",
    "            predicted_labels = (logits > 0.5).reshape(-1)\n",
    "            all_preds = torch.cat((all_preds, predicted_labels), 0).reshape(-1)\n",
    "\n",
    "        # Move to CPU\n",
    "        all_preds, all_labs = all_preds.cpu(), all_labs.cpu()\n",
    "        accuracy = round(accuracy_score(all_labs, all_preds), 4) # of all predictions, how many were correct?\n",
    "        precision = round(precision_score(all_labs, all_preds), 4) # of predicted anomalies, how many were correct?\n",
    "        recall = round(recall_score(all_labs, all_preds), 4) # of all true anomalies, how many were recovered?\n",
    "\n",
    "        if print_output:\n",
    "            print(confusion_matrix(all_labs, all_preds), end = '\\n\\n')\n",
    "            print('Accuracy:', accuracy) \n",
    "            print('Precision:', precision) \n",
    "            print('Recall:', recall)\n",
    "            return None\n",
    "        \n",
    "        return accuracy, precision, recall\n",
    "\n",
    "################################\n",
    "## DEFINE TRAINING + TESTING FUNCTION\n",
    "################################\n",
    "def training_function():\n",
    "    print('\\nSTARTING TRAINING\\n')\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "        print(f'Current Epoch: {epoch+1}')\n",
    "\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "\n",
    "            inpt, labs = batch_data[0], batch_data[1].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad() # Clear gradients from last batch\n",
    "\n",
    "            logits = model(inpt) # Compute logits (ie. final hidden state)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = criterion(logits.squeeze(), labs.float())\n",
    "            cost.backward()\n",
    "\n",
    "            # Use clip_grad_norm to prevent exploding gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log\n",
    "            if not batch_idx % 100:\n",
    "                print (f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                       f'BCE Cost: {cost:.4f}')\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            model.eval()\n",
    "            print(f'Epoch Comleted: {epoch+1}'\n",
    "                  f'\\n\\twith training accuracy: {compute_binary_accuracy(model, train_loader, DEVICE)[0]:.4f}%'\n",
    "                  f'\\n\\tand validation accuracy: {compute_binary_accuracy(model, val_loader, DEVICE)[0]:.4f}%')\n",
    "\n",
    "        print(f'\\tTime elapsed: {(time.time() - start_time)/60:.2f} min\\n')\n",
    "\n",
    "    ## Final Test Accuracy\n",
    "    print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "    print('----------------------------------------------')\n",
    "    print('Final Test Accuracy/Precision/Recall : ', compute_binary_accuracy(model, test_loader, DEVICE))\n",
    "    print('----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make models reproducible\n",
    "RANDOM_SEED = 123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "Current Epoch: 1\n",
      "Batch 000/402 | BCE Cost: 0.6704\n",
      "Batch 100/402 | BCE Cost: 0.1283\n",
      "Batch 200/402 | BCE Cost: 0.0858\n",
      "Batch 300/402 | BCE Cost: 0.0890\n",
      "Batch 400/402 | BCE Cost: 0.1065\n",
      "Epoch Comleted: 1\n",
      "\twith training accuracy: 0.9815%\n",
      "\tand validation accuracy: 0.9816%\n",
      "\tTime elapsed: 0.85 min\n",
      "\n",
      "Current Epoch: 2\n",
      "Batch 000/402 | BCE Cost: 0.0928\n",
      "Batch 100/402 | BCE Cost: 0.0595\n",
      "Batch 200/402 | BCE Cost: 0.0224\n",
      "Batch 300/402 | BCE Cost: 0.0107\n",
      "Batch 400/402 | BCE Cost: 0.0055\n",
      "Epoch Comleted: 2\n",
      "\twith training accuracy: 0.9981%\n",
      "\tand validation accuracy: 0.9980%\n",
      "\tTime elapsed: 1.65 min\n",
      "\n",
      "Current Epoch: 3\n",
      "Batch 000/402 | BCE Cost: 0.0112\n",
      "Batch 100/402 | BCE Cost: 0.0074\n",
      "Batch 200/402 | BCE Cost: 0.0180\n",
      "Batch 300/402 | BCE Cost: 0.0044\n",
      "Batch 400/402 | BCE Cost: 0.0152\n",
      "Epoch Comleted: 3\n",
      "\twith training accuracy: 0.9989%\n",
      "\tand validation accuracy: 0.9987%\n",
      "\tTime elapsed: 2.45 min\n",
      "\n",
      "Total Training Time: 2.45 min\n",
      "----------------------------------------------\n",
      "Final Test Accuracy/Precision/Recall :  (0.9989, 0.9798, 0.9827)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Params: VANILLA\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 3\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "## INITIALIZE MODEL\n",
    "model = Anomaly_Detector(VOCAB_SIZE, OUTPUT_SIZE, EMBEDDING_DIM, HIDDEN_DIM, BATCH_SIZE, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "## LOSS AND OPTIMIZER\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "## TRAIN/TEST\n",
    "training_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "Current Epoch: 1\n",
      "Batch 000/402 | BCE Cost: 0.6993\n",
      "Batch 100/402 | BCE Cost: 0.0863\n",
      "Batch 200/402 | BCE Cost: 0.1178\n",
      "Batch 300/402 | BCE Cost: 0.0860\n",
      "Batch 400/402 | BCE Cost: 0.0934\n",
      "Epoch Comleted: 1\n",
      "\twith training accuracy: 0.9815%\n",
      "\tand validation accuracy: 0.9808%\n",
      "\tTime elapsed: 1.85 min\n",
      "\n",
      "Current Epoch: 2\n",
      "Batch 000/402 | BCE Cost: 0.0537\n",
      "Batch 100/402 | BCE Cost: 0.0155\n",
      "Batch 200/402 | BCE Cost: 0.0071\n",
      "Batch 300/402 | BCE Cost: 0.0072\n",
      "Batch 400/402 | BCE Cost: 0.0089\n",
      "Epoch Comleted: 2\n",
      "\twith training accuracy: 0.9981%\n",
      "\tand validation accuracy: 0.9982%\n",
      "\tTime elapsed: 3.88 min\n",
      "\n",
      "Current Epoch: 3\n",
      "Batch 000/402 | BCE Cost: 0.0061\n",
      "Batch 100/402 | BCE Cost: 0.0168\n",
      "Batch 200/402 | BCE Cost: 0.0101\n",
      "Batch 300/402 | BCE Cost: 0.0088\n",
      "Batch 400/402 | BCE Cost: 0.0032\n",
      "Epoch Comleted: 3\n",
      "\twith training accuracy: 0.9989%\n",
      "\tand validation accuracy: 0.9990%\n",
      "\tTime elapsed: 5.93 min\n",
      "\n",
      "Total Training Time: 5.93 min\n",
      "----------------------------------------------\n",
      "Final Test Accuracy/Precision/Recall :  (0.9989, 0.9801, 0.9813)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Params: 2 Layer\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 3\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "## INITIALIZE MODEL\n",
    "model = Anomaly_Detector(VOCAB_SIZE, OUTPUT_SIZE, EMBEDDING_DIM, HIDDEN_DIM, BATCH_SIZE, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "## LOSS AND OPTIMIZER\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "training_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "Current Epoch: 1\n",
      "Batch 000/402 | BCE Cost: 0.7502\n",
      "Batch 100/402 | BCE Cost: 0.1390\n",
      "Batch 200/402 | BCE Cost: 0.1049\n",
      "Batch 300/402 | BCE Cost: 0.0787\n",
      "Batch 400/402 | BCE Cost: 0.0833\n",
      "Epoch Comleted: 1\n",
      "\twith training accuracy: 0.9815%\n",
      "\tand validation accuracy: 0.9809%\n",
      "\tTime elapsed: 0.90 min\n",
      "\n",
      "Current Epoch: 2\n",
      "Batch 000/402 | BCE Cost: 0.1048\n",
      "Batch 100/402 | BCE Cost: 0.0760\n",
      "Batch 200/402 | BCE Cost: 0.0169\n",
      "Batch 300/402 | BCE Cost: 0.0080\n",
      "Batch 400/402 | BCE Cost: 0.0135\n",
      "Epoch Comleted: 2\n",
      "\twith training accuracy: 0.9980%\n",
      "\tand validation accuracy: 0.9982%\n",
      "\tTime elapsed: 1.79 min\n",
      "\n",
      "Current Epoch: 3\n",
      "Batch 000/402 | BCE Cost: 0.0170\n",
      "Batch 100/402 | BCE Cost: 0.0036\n",
      "Batch 200/402 | BCE Cost: 0.0027\n",
      "Batch 300/402 | BCE Cost: 0.0169\n",
      "Batch 400/402 | BCE Cost: 0.0020\n",
      "Epoch Comleted: 3\n",
      "\twith training accuracy: 0.9991%\n",
      "\tand validation accuracy: 0.9991%\n",
      "\tTime elapsed: 2.68 min\n",
      "\n",
      "Total Training Time: 2.68 min\n",
      "----------------------------------------------\n",
      "Final Test Accuracy/Precision/Recall :  (0.9991, 0.993, 0.9777)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Params 1 Layer / Bidirectional\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 3\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "## INITIALIZE MODEL\n",
    "model = Anomaly_Detector(VOCAB_SIZE, OUTPUT_SIZE, EMBEDDING_DIM, HIDDEN_DIM, BATCH_SIZE, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "## LOSS AND OPTIMIZER\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "training_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "Current Epoch: 1\n",
      "Batch 000/402 | BCE Cost: 0.7074\n",
      "Batch 100/402 | BCE Cost: 0.0932\n",
      "Batch 200/402 | BCE Cost: 0.1304\n",
      "Batch 300/402 | BCE Cost: 0.0753\n",
      "Batch 400/402 | BCE Cost: 0.0239\n",
      "Epoch Comleted: 1\n",
      "\twith training accuracy: 0.9816%\n",
      "\tand validation accuracy: 0.9813%\n",
      "\tTime elapsed: 2.35 min\n",
      "\n",
      "Current Epoch: 2\n",
      "Batch 000/402 | BCE Cost: 0.0371\n",
      "Batch 100/402 | BCE Cost: 0.0210\n",
      "Batch 200/402 | BCE Cost: 0.0154\n",
      "Batch 300/402 | BCE Cost: 0.0086\n",
      "Batch 400/402 | BCE Cost: 0.0021\n",
      "Epoch Comleted: 2\n",
      "\twith training accuracy: 0.9983%\n",
      "\tand validation accuracy: 0.9982%\n",
      "\tTime elapsed: 4.21 min\n",
      "\n",
      "Current Epoch: 3\n",
      "Batch 000/402 | BCE Cost: 0.0077\n",
      "Batch 100/402 | BCE Cost: 0.0051\n",
      "Batch 200/402 | BCE Cost: 0.0034\n",
      "Batch 300/402 | BCE Cost: 0.0010\n",
      "Batch 400/402 | BCE Cost: 0.0008\n",
      "Epoch Comleted: 3\n",
      "\twith training accuracy: 0.9995%\n",
      "\tand validation accuracy: 0.9996%\n",
      "\tTime elapsed: 6.05 min\n",
      "\n",
      "Total Training Time: 6.05 min\n",
      "----------------------------------------------\n",
      "Final Test Accuracy/Precision/Recall :  (0.9995, 0.9943, 0.9895)\n",
      "----------------------------------------------\n",
      "[[111632     19]\n",
      " [    35   3314]]\n",
      "\n",
      "Accuracy: 0.9995\n",
      "Precision: 0.9943\n",
      "Recall: 0.9895\n"
     ]
    }
   ],
   "source": [
    "# Params: 2 layer / Bidirectional\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 3\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "## INITIALIZE MODEL\n",
    "model = Anomaly_Detector(VOCAB_SIZE, OUTPUT_SIZE, EMBEDDING_DIM, HIDDEN_DIM, BATCH_SIZE, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "## LOSS AND OPTIMIZER\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "training_function()\n",
    "\n",
    "## See confusion matrix\n",
    "compute_binary_accuracy(model, test_loader, DEVICE, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0b49af92716124a3dcdffffc9745ca4adb6a99ad9a5084f8f6ea1cee1a8d52ff7",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}