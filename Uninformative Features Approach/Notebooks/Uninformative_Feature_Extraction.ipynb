{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "precious-fifty",
   "metadata": {},
   "source": [
    "# Uninformative Feature Extraction Notebook\n",
    "\n",
    "### Log Structure\n",
    "The HDFS log is built the following way:\n",
    "- %d{yy/MM/dd HH:mm:ss}: The date and time\n",
    "- %???: Unknown 2-4 digit code\n",
    "- %p: The priority of the logging event (INFO, WARN, DEBUG, ERROR, etc.)\n",
    "- %c: Category of logging event (class name)\n",
    "- %m: Log message which may or may not contain a block (blk_ID)\n",
    "\n",
    "\n",
    "### Uninformative Features\n",
    "\n",
    "We will manually extract the following features by parsing the raw log events:\n",
    "- Event datetime \n",
    "- Event anomaly label (used an external file with labeled block IDs)\n",
    "- Event priority\n",
    "- Event category\n",
    "- Words per event\n",
    "- Time interval between events\n",
    "- Events in rolling time window (10min, 1min, 1s)\n",
    "- One hot encoded words in log message\n",
    "\n",
    "Further feature detection will be done in an unsupervised way using autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-princess",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "creative-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from datetime import datetime as dt\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-helping",
   "metadata": {},
   "source": [
    "## Import Raw Data\n",
    "This section can be skipped since exported CSV features already exist.\n",
    "\n",
    "**Go directly to the import dataframe section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "swiss-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Raw Log Data\n",
    "raw = pd.read_csv('Data/HDFS.log', header=None, sep='\\n')[0]\n",
    "\n",
    "## Import code testing data sets\n",
    "#raw = raw[0][:20000] # FOR TESTING\n",
    "#raw = pd.read_csv('Data/HDFS_2k.log', header = None)[0] # FOR TESTING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-reach",
   "metadata": {},
   "source": [
    "## Code Parsing Section\n",
    "\n",
    "The following parsing cells should only be run **ONCE**, then all data exported to CSV for quick imports and further data manipulation if necessary.\n",
    "\n",
    "Since the CSV files exist locally, we skip to the import section for secondary feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-mouse",
   "metadata": {},
   "source": [
    "### Get BLock IDs and their Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "restricted-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len labels:  575061\n",
      "Anomaly count:  16838\n",
      "% Anomalous:  2.93 %\n"
     ]
    }
   ],
   "source": [
    "## Import Anomaly Labels\n",
    "labels = pd.read_csv('Data/anomaly_label.csv')\n",
    "labels.iloc[:,1][labels.Label == 'Anomaly'] = 1\n",
    "labels.iloc[:,1][labels.Label == 'Normal'] = 0\n",
    "\n",
    "length = len(labels)\n",
    "anomalies = len(labels[labels.Label==1])\n",
    "print('Len labels: ', length)\n",
    "print('Anomaly count: ', anomalies)\n",
    "print('% Anomalous: ', round(anomalies/length*100, 2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-invasion",
   "metadata": {},
   "source": [
    "### Parse for Log Event BlockIDs (and label them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hearing-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Blocks from Log\n",
    "blocks_in_order = raw.str.findall(r'(blk_.[\\d]*)')\n",
    "unlist_vectorized = np.vectorize(lambda x: x[0])\n",
    "blocks_in_order = pd.Series(unlist_vectorized(blocks_in_order))\n",
    "\n",
    "\n",
    "## Label the extracted block ids via conversion dictionary\n",
    "binarizer = dict(zip(labels.BlockId, labels.Label))\n",
    "binarizer_vectorized = np.vectorize(lambda x: binarizer[x])\n",
    "blocks_binarized = pd.Series(binarizer_vectorized(blocks_in_order))\n",
    "\n",
    "## Create export checkpoint\n",
    "labeled_blks = pd.DataFrame({'blkID':blocks_in_order, 'anomaly':blocks_binarized})\n",
    "labeled_blks.head()\n",
    "labeled_blks.to_feather('Data/labeled_blks.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-continent",
   "metadata": {},
   "source": [
    "### Parse for Message Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "administrative-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Messages\n",
    "full_messages = raw.str.extract(r'((?<=:\\s).*)')[0]\n",
    "\n",
    "## Extract Key Words by Event\n",
    "message_keywords = full_messages.str.findall(r'([A-Za-z]+)') \n",
    "\n",
    "## Extract Features\n",
    "words_per_event = message_keywords.map(len)\n",
    "all_words = pd.Series(itertools.chain(*message_keywords))\n",
    "unique_words = pd.Series(all_words.unique())\n",
    "\n",
    "## Export to CSV for processing in Databricks / Cloud\n",
    "message_keywords.to_csv(r'Data/message_keywords.csv', index = False, header = False)\n",
    "words_per_event.to_csv(r'Data/words_per_event.csv', index = False, header = False)\n",
    "all_words.to_csv(r'Data/all_words.csv', index = False, header = False)\n",
    "unique_words.to_csv(r'Data/unique_words.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-cooperation",
   "metadata": {},
   "source": [
    "### Parse for Event Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw categories\n",
    "category = raw.str.extract(r'((?<=dfs\\.)[^:]*)')[0]\n",
    "cat_types = sorted(category.unique())\n",
    "cat_num = len(category)\n",
    "\n",
    "# Conversion dictionary function\n",
    "cat_converter = {key:value for (key, value) in zip(cat_types, np.arange(0, cat_num))}\n",
    "cat_converter_vectorized = np.vectorize(lambda x: cat_converter[x])\n",
    "\n",
    "# Convert\n",
    "category_numeric = pd.Series(cat_converter_vectorized(category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-japan",
   "metadata": {},
   "source": [
    "### Parse for Event Priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw priorities\n",
    "priority = raw.str.extract(r'([A-Z]{2,})')[0]\n",
    "prio_types = sorted(priority.unique())\n",
    "prio_num = len(priority)\n",
    "\n",
    "# Conversion dictionary function\n",
    "prio_converter = {key:value for (key, value) in zip(prio_types, np.arange(0, prio_num))}\n",
    "prio_converter_vectorized = np.vectorize(lambda x: prio_converter[x])\n",
    "\n",
    "# Convert\n",
    "priority_numeric = pd.Series(prio_converter_vectorized(priority))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-serial",
   "metadata": {},
   "source": [
    "*Quick check to ensure all rows were captured and what the category/priority types were*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Check\n",
    "raw_num = len(raw)\n",
    "\n",
    "print('Categories :', cat_converter,\n",
    "      '\\nNumber of Categories :', len(category.unique()),\n",
    "      '\\n\\nPriorities :', prio_converter,\n",
    "      '\\nNumber of Priorities :', len(priority.unique()),\n",
    "      f'\\n\\nCheck all {raw_num} rows were captured:', (raw_num == cat_num) == (raw_num == prio_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-finnish",
   "metadata": {},
   "source": [
    "### Parse for DateTime Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetimes from Log\n",
    "dt_series = raw.str[0:13]\n",
    "\n",
    "# Convert to DateTime bbjects\n",
    "dt_converter1 = np.vectorize(lambda x: dt.strptime(x,'%y%m%d %H%M%S'))\n",
    "datetime = pd.Series(dt_converter(dt_series))\n",
    "\n",
    "# Convert to DateTime Objects if importing from csv?\n",
    "#dt_converter2 = np.vectorize(lambda x: dt.fromisoformat(x))\n",
    "#datetime = pd.Series(dt_converter(datetime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-andorra",
   "metadata": {},
   "source": [
    "### Extract time intervals and events inside rolling time window\n",
    "    2) Extract word features. We'll deal with that later as there are a plethora of options for this (word ratios, one hot encoding for word appearances, atypical word selection etc.).\n",
    "    3) One hot encode the message keywords.\n",
    "\n",
    "For now, let's import processed data and create our numeric dataframe.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "arabic-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT HERE\n",
    "datetime = pd.read_csv('Data/features/datetime.csv', header = None, names = ['datetime'], squeeze = True)\n",
    "\n",
    "\n",
    "#blocks_binarized = pd.read_csv('Data/features/anomaly_labels.csv', header = None, names = ['anomaly'], squeeze = True)\n",
    "#priority_numeric = pd.read_csv('Data/features/priority_numeric.csv', header = None, names = ['priority'], squeeze = True)\n",
    "#category_numeric = pd.read_csv('Data/features/category_numeric.csv', header = None, names = ['category'], squeeze = True)\n",
    "#words_per_event = pd.read_csv('Data/features/words_per_event.csv', header = None, names = ['words_per_event'], squeeze = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "centered-nation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anomaly</th>\n",
       "      <th>priority</th>\n",
       "      <th>category</th>\n",
       "      <th>words_per_event</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11-11 06:52:05</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-11 07:45:32</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-09 20:56:16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     anomaly  priority  category  words_per_event\n",
       "datetime                                                         \n",
       "2008-11-11 06:52:05        0       0.0       7.0              9.0\n",
       "2008-11-11 07:45:32        0       0.0       5.0              6.0\n",
       "2008-11-09 20:56:16        0       0.0       7.0             11.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create numeric dataframe\n",
    "log_numeric = pd.DataFrame({'datetime':datetime,\n",
    "                            'anomaly':blocks_binarized,\n",
    "                            'priority':priority_numeric,\n",
    "                            'category':category_numeric,\n",
    "                            'words_per_event':words_per_event,\n",
    "                            # WORD FEATURES\n",
    "                            })\n",
    "\n",
    "log_numeric = log_numeric.set_index('datetime')\n",
    "log_numeric.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-romantic",
   "metadata": {},
   "source": [
    "### Compute Time Interval Features\n",
    "\n",
    "Here we add a column for the time intervals between events so the model has time separation features (ie. how far apart events occur)\n",
    "\n",
    "We also add a column for the number of events that occur within a 10 minute, 1 minute, and 1 second rolling window.\n",
    "\n",
    "**Numeric Log Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dress-whale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anomaly</th>\n",
       "      <th>priority</th>\n",
       "      <th>category</th>\n",
       "      <th>words_per_event</th>\n",
       "      <th>delta_t</th>\n",
       "      <th>evnts_in_10min</th>\n",
       "      <th>evnts_in_1min</th>\n",
       "      <th>evnts_in_1s</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11-10 22:57:45</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>76425</td>\n",
       "      <td>7442</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-09 23:29:34</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>72576</td>\n",
       "      <td>6296</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-09 20:48:57</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>77131</td>\n",
       "      <td>7318</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-10 21:02:19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>256940</td>\n",
       "      <td>11983</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-11 06:51:56</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>274995</td>\n",
       "      <td>111028</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-11 09:44:32</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>78677</td>\n",
       "      <td>8610</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-10 13:56:08</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9201</td>\n",
       "      <td>1658</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-10 21:06:18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>109327</td>\n",
       "      <td>30541</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-11 06:52:12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>235767</td>\n",
       "      <td>72427</td>\n",
       "      <td>1809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-11 04:47:28</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>160302</td>\n",
       "      <td>69467</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     anomaly  priority  category  words_per_event  delta_t  \\\n",
       "datetime                                                                     \n",
       "2008-11-10 22:57:45        0       0.0       5.0              6.0        0   \n",
       "2008-11-09 23:29:34        0       0.0       5.0              5.0        0   \n",
       "2008-11-09 20:48:57        0       0.0       4.0              5.0        0   \n",
       "2008-11-10 21:02:19        0       0.0       6.0             11.0        0   \n",
       "2008-11-11 06:51:56        0       0.0       7.0              9.0        0   \n",
       "2008-11-11 09:44:32        0       0.0       7.0             11.0        0   \n",
       "2008-11-10 13:56:08        0       0.0       4.0              4.0        0   \n",
       "2008-11-10 21:06:18        0       0.0       6.0             11.0        0   \n",
       "2008-11-11 06:52:12        0       0.0       7.0              9.0        0   \n",
       "2008-11-11 04:47:28        0       0.0       6.0             11.0        0   \n",
       "\n",
       "                     evnts_in_10min  evnts_in_1min  evnts_in_1s  \n",
       "datetime                                                         \n",
       "2008-11-10 22:57:45           76425           7442           60  \n",
       "2008-11-09 23:29:34           72576           6296           64  \n",
       "2008-11-09 20:48:57           77131           7318           58  \n",
       "2008-11-10 21:02:19          256940          11983           10  \n",
       "2008-11-11 06:51:56          274995         111028          375  \n",
       "2008-11-11 09:44:32           78677           8610            5  \n",
       "2008-11-10 13:56:08            9201           1658           12  \n",
       "2008-11-10 21:06:18          109327          30541          146  \n",
       "2008-11-11 06:52:12          235767          72427         1809  \n",
       "2008-11-11 04:47:28          160302          69467          435  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get Time Intervals, delta_t\n",
    "delta_t = datetime-datetime.shift()\n",
    "delta_t[0] = delta_t[1] - delta_t[1] # set first item to zero\n",
    "delta_t = np.array(delta_t / np.timedelta64(1, 's'), dtype = int) # convert to seconds\n",
    "log_numeric['delta_t'] = delta_t\n",
    "\n",
    "## Compute Event Counts in Rolling Windos\n",
    "\n",
    "## Problem: pd.rolling only looks backwards --> we want a forward look ahead and we don't want diminishing counts near the end of the dataframe if all events occured within a few seconds.\n",
    "# Solution Part 1: apply rolling on the reverse series then reverse the answer\n",
    "# Solution Part 2: add 100,000 extra rows (or more) with a datetime > 600s so the rolling window includes all events at the starting window\n",
    "\n",
    "# add temporary rows\n",
    "rows_added = 100000\n",
    "cols = log_numeric.columns.tolist()\n",
    "template_row = pd.DataFrame(np.full((1, len(cols)), 0), columns = cols, index = [dt(2100,1,1,0,0,0)])\n",
    "template_row.index.name = 'datetime'\n",
    "log_numeric = log_numeric.append(template_row.append([template_row]*rows_added)) \n",
    "\n",
    "# counting\n",
    "counts_600s = log_numeric.delta_t[::-1].rolling('600s').count()[::-1]\n",
    "counts_60s = log_numeric.delta_t[::-1].rolling('60s').count()[::-1]\n",
    "counts_1s = log_numeric.delta_t[::-1].rolling('1s').count()[::-1]\n",
    "\n",
    "# keep relevant counts\n",
    "counts_600s = counts_600s[:-(rows_added+1)]\n",
    "counts_60s = counts_60s[:-(rows_added+1)]\n",
    "counts_1s = counts_1s[:-(rows_added+1)]\n",
    "\n",
    "# remove temporary rows\n",
    "log_numeric = log_numeric.iloc[:-(rows_added+1), :]\n",
    "\n",
    "# add count features\n",
    "log_numeric['evnts_in_10min'] = np.array(counts_600s, dtype = int) \n",
    "log_numeric['evnts_in_1min'] = np.array(counts_60s, dtype = int) \n",
    "log_numeric['evnts_in_1s'] = np.array(counts_1s, dtype = int) \n",
    "\n",
    "\n",
    "## Export Numeric Log\n",
    "log_numeric.to_csv(r'Data/log_numeric.csv', index = True, header = True)\n",
    "log_numeric.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-angel",
   "metadata": {},
   "source": [
    "**Numeric Log Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "parental-clearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anomaly</th>\n",
       "      <th>words_per_event</th>\n",
       "      <th>delta_t</th>\n",
       "      <th>evnts_in_10min</th>\n",
       "      <th>evnts_in_1min</th>\n",
       "      <th>evnts_in_1s</th>\n",
       "      <th>prio_0</th>\n",
       "      <th>prio_1</th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "      <th>cat_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11-11 02:30:37</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>140322</td>\n",
       "      <td>12718</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-10 21:01:27</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>393924</td>\n",
       "      <td>144204</td>\n",
       "      <td>2719</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-10 11:10:03</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>78798</td>\n",
       "      <td>7868</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-11 02:39:42</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>76474</td>\n",
       "      <td>7777</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-11 07:56:27</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>229848</td>\n",
       "      <td>63432</td>\n",
       "      <td>2463</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-10 15:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>83064</td>\n",
       "      <td>8192</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-10 02:03:28</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>28673</td>\n",
       "      <td>6577</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-09 21:02:35</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>78261</td>\n",
       "      <td>7418</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-11 04:43:22</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>378858</td>\n",
       "      <td>122551</td>\n",
       "      <td>480</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-10 01:15:51</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>75423</td>\n",
       "      <td>7594</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     anomaly  words_per_event  delta_t  evnts_in_10min  \\\n",
       "datetime                                                                 \n",
       "2008-11-11 02:30:37        0                9        0          140322   \n",
       "2008-11-10 21:01:27        0                9        0          393924   \n",
       "2008-11-10 11:10:03        0                6        0           78798   \n",
       "2008-11-11 02:39:42        0               10        0           76474   \n",
       "2008-11-11 07:56:27        0                9        0          229848   \n",
       "2008-11-10 15:00:00        0                6        0           83064   \n",
       "2008-11-10 02:03:28        0                5        0           28673   \n",
       "2008-11-09 21:02:35        0                5        0           78261   \n",
       "2008-11-11 04:43:22        0                9        0          378858   \n",
       "2008-11-10 01:15:51        0                5        0           75423   \n",
       "\n",
       "                     evnts_in_1min  evnts_in_1s  prio_0  prio_1  cat_0  cat_1  \\\n",
       "datetime                                                                        \n",
       "2008-11-11 02:30:37          12718          110       1       0      0      0   \n",
       "2008-11-10 21:01:27         144204         2719       1       0      0      0   \n",
       "2008-11-10 11:10:03           7868           49       1       0      0      0   \n",
       "2008-11-11 02:39:42           7777            7       1       0      0      0   \n",
       "2008-11-11 07:56:27          63432         2463       1       0      0      0   \n",
       "2008-11-10 15:00:00           8192           40       1       0      0      0   \n",
       "2008-11-10 02:03:28           6577           59       1       0      0      0   \n",
       "2008-11-09 21:02:35           7418          100       1       0      0      0   \n",
       "2008-11-11 04:43:22         122551          480       1       0      0      0   \n",
       "2008-11-10 01:15:51           7594           48       1       0      0      0   \n",
       "\n",
       "                     cat_2  cat_3  cat_4  cat_5  cat_6  cat_7  cat_8  \n",
       "datetime                                                              \n",
       "2008-11-11 02:30:37      0      0      0      0      0      1      0  \n",
       "2008-11-10 21:01:27      0      0      0      0      0      1      0  \n",
       "2008-11-10 11:10:03      0      0      0      1      0      0      0  \n",
       "2008-11-11 02:39:42      0      0      0      0      0      1      0  \n",
       "2008-11-11 07:56:27      0      0      0      0      0      1      0  \n",
       "2008-11-10 15:00:00      0      0      0      1      0      0      0  \n",
       "2008-11-10 02:03:28      0      0      1      0      0      0      0  \n",
       "2008-11-09 21:02:35      0      0      0      1      0      0      0  \n",
       "2008-11-11 04:43:22      0      0      0      0      0      1      0  \n",
       "2008-11-10 01:15:51      0      0      1      0      0      0      0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import Log numeric if wanting to start here\n",
    "log_numeric = pd.read_csv('Data/dataframes/log_numeric.csv', index_col = 'datetime')\n",
    "\n",
    "## One-Hot encode the dataframe\n",
    "log_onehot_no_words = pd.get_dummies(log_numeric, columns=['priority','category'], prefix=['prio', 'cat'])\n",
    "\n",
    "## Export to CSV\n",
    "log_onehot_no_words.to_csv(r'Data/log_onehot_no_words.csv', index = True, header = True)\n",
    "\n",
    "log_onehot_no_words.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-warrior",
   "metadata": {},
   "source": [
    "**One Hot Log (no words) Checkpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-embassy",
   "metadata": {},
   "source": [
    "***\n",
    "END OF PARSING\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-block",
   "metadata": {},
   "source": [
    "Certain columns (priority, category) have more than 2 categorical states (0,1,...,n). To make the categorical data useable for machines, we one-hot encode those variables.\n",
    "\n",
    "Finally we export our dataframe to CSV.\n",
    "\n",
    "We will still need to futher process the data based on the model we choose. If we take a time series approach we will need to create a rolling window with matrix information of all variables. \n",
    "\n",
    "*Note: current data did not distinguish anomalies from normal events after a PCA analysis. Lets try looking at word features.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-rider",
   "metadata": {},
   "source": [
    "## Tertiary Feature Extraction (Message Keywords)\n",
    "\n",
    "This was a desperate attempt to extract keywords deemed relevant. The amount of time spent doing this was kept to a minimal so more \"computationally clever\" code is not exactly present in this memory heavy cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "determined-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import parsed keywords (lists in string form)\n",
    "#string_message_keywords = pd.read_csv('Data/features/message_keywords.csv', header = None, names = ['words'], squeeze = True)\n",
    "\n",
    "## manually entered an exclusion based on 166 unique words found. 111 words (ie. columns) remain\n",
    "exclusion_set = {'block','blk','src','dest','BLOCK','mnt','PacketResponder','for','of','size','from','is','to','datanode','s','user','root','ip','ec','task','m','part','java','net','io','Could','dfs','data','packet','in','by','on','while','be','ch','nio','IO','meta','pipe','and','got','volumeMap','But','it','does','any','sortrand','r','grep','grepa','sort','grepb','as','an','The'}\n",
    "\n",
    "#### old\n",
    "#str_to_list = np.vectorize(lambda x: np.array(x.strip(\"[]'\").split(\"', '\").remove(), object))\n",
    "#message_keywords = pd.Series(str_to_list(string_message_keywords)) # This line takes ~ 20 seconds\n",
    "#one_hot_keywords = message_keywords.str.join('|').str.get_dummies() # This line takes ~ 30+ min\n",
    "\n",
    "#### new\n",
    "## Convert back to list\n",
    "## vectorized ((str to list) - excluded keywords) function\n",
    "str_to_list = np.vectorize(lambda x: set(x.strip(\"[]'\").split(\"', '\")) - exclusion_set)\n",
    "\n",
    "## One hot encoding\n",
    "## THIS LINE CRASHES WINDOWS WHEN RUN ON THE WHOLE SET\n",
    "#keywords_onehot = pd.Series(str_to_list(string_message_keywords)).str.join('|').str.get_dummies()\n",
    "\n",
    "## Solution: partition data by 2 million rows, stack, remove from memory, repeat!\n",
    "## DONT RUN THIS AGAIN UNLESS YOU NEED TO\n",
    "\n",
    "#part1 = pd.Series(str_to_list(string_message_keywords[0:2000000])).str.join('|').str.get_dummies()\n",
    "#part2 = pd.Series(str_to_list(string_message_keywords[2000000:4000000])).str.join('|').str.get_dummies()\n",
    "#latest_agg = pd.concat([part1, part2], axis=0, join='outer').replace(np.nan, 0).astype(int)\n",
    "#del part1; del part2\n",
    "\n",
    "#part3 = pd.Series(str_to_list(string_message_keywords[4000000:6000000])).str.join('|').str.get_dummies()\n",
    "#latest_agg = pd.concat([latest_agg, part3], axis=0, join='outer').replace(np.nan, 0).astype(int)\n",
    "#del part3\n",
    "\n",
    "#part4 = pd.Series(str_to_list(string_message_keywords[6000000:8000000])).str.join('|').str.get_dummies()\n",
    "#latest_agg = pd.concat([latest_agg, part4], axis=0, join='outer').replace(np.nan, 0).astype(int)\n",
    "#del part4\n",
    "\n",
    "#part5 = pd.Series(str_to_list(string_message_keywords[8000000:10000000])).str.join('|').str.get_dummies()\n",
    "#latest_agg = pd.concat([latest_agg, part5], axis=0, join='outer').replace(np.nan, 0).astype(int)\n",
    "#del part5\n",
    "\n",
    "#part6 = pd.Series(str_to_list(string_message_keywords[10000000:])).str.join('|').str.get_dummies()\n",
    "#latest_agg = pd.concat([latest_agg, part6], axis=0, join='outer').replace(np.nan, 0).astype(int)\n",
    "#del part6\n",
    "\n",
    "## SAVE\n",
    "#latest_agg.to_csv(r'Data/keywords_onehot.csv', header = True)\n",
    "#latest_agg.reset_index().to_feather(r'Data/keywords_onehot.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-prefix",
   "metadata": {},
   "source": [
    "***\n",
    "END OF FEATURE EXTRACTION\n",
    "\n",
    "START OF DATAFRAME JOINING (our two checkpoint dataframes)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-beginning",
   "metadata": {},
   "source": [
    "## Combine Primary Feature Dataframes\n",
    "\n",
    "1) One hot encoded non-message features\n",
    "\n",
    "2) One hot encoded message keyword features\n",
    "\n",
    "A combined dataframe with column names wasn't saved because we won't necessarily need the column names when feeding it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acoustic-magnet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11175629 entries, 0 to 11175628\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Dtype\n",
      "---  ------           -----\n",
      " 0   words_per_event  int64\n",
      " 1   delta_t          int64\n",
      " 2   evnts_in_10min   int64\n",
      " 3   evnts_in_1min    int64\n",
      " 4   evnts_in_1s      int64\n",
      " 5   prio_0           int64\n",
      " 6   prio_1           int64\n",
      " 7   cat_0            int64\n",
      " 8   cat_1            int64\n",
      " 9   cat_2            int64\n",
      " 10  cat_3            int64\n",
      " 11  cat_4            int64\n",
      " 12  cat_5            int64\n",
      " 13  cat_6            int64\n",
      " 14  cat_7            int64\n",
      " 15  cat_8            int64\n",
      "dtypes: int64(16)\n",
      "memory usage: 1.3 GB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11175629 entries, 0 to 11175628\n",
      "Columns: 112 entries, index to route\n",
      "dtypes: int32(111), int64(1)\n",
      "memory usage: 4.7 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Import the most recent onehot log\n",
    "log_onehot_no_words = pd.read_csv('Data/pre_combined_dataframes/log_onehot_no_words.csv').iloc[:,2:] # dont include datetime or anomaly column in final output\n",
    "print(log_onehot_no_words.info())\n",
    "## Import the keyword onehot dataframe\n",
    "log_onehot_keywords = pd.read_feather('Data/pre_combined_dataframes/keywords_onehot.feather')\n",
    "print(log_onehot_keywords.info())\n",
    "\n",
    "\n",
    "## Check shape compatability\n",
    "if (log_onehot_no_words.shape[0] == log_onehot_keywords.shape[0]):\n",
    "    ## Check indeces are identical\n",
    "    if (len(pd.Series(log_onehot_no_words.index == log_onehot_keywords.index).unique()) == 1):\n",
    "        ## COMBINE\n",
    "        log_onehot_complete = log_onehot_no_words.join(log_onehot_keywords)\n",
    "        ## Scale\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled = scaler.fit_transform(log_onehot_complete); del log_onehot_complete\n",
    "        ## Export sparse array to feather / no datetime \n",
    "        np.save('Data/ONEHOT_SCALED.npy', scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "least-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11175629 entries, 0 to 11175628\n",
      "Columns: 112 entries, index to route\n",
      "dtypes: int32(111), int64(1)\n",
      "memory usage: 4.7 GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.051384452, 'GiB')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(log_onehot_keywords.info())\n",
    "sys.getsizeof(log_onehot_keywords)/1000000000, 'GiB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-secretary",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sliding Window Matrix Data\n",
    "\n",
    "Unfortunately, the export for the sliding window dataframe doens't work since the file size exceeds the amount of RAM my computer has. Fortunately, this cell doesn't take a long time to run. As such, we will reuse this code to create the windows in the modelling file where any final pre processing occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load checkpoint\n",
    "scaled = np.load('Data/ONEHOT_SCALED.npy')\n",
    "\n",
    "window_size = 10\n",
    "windows = []\n",
    "\n",
    "## Make Windows\n",
    "windows = [scaled[row:row+window_size] for row in range(scaled.shape[0]-window_size)]\n",
    "\n",
    "print('Object: windows\\nSize:', sys.getsizeof(windows)/1000000000, 'GiB')\n",
    "\n",
    "## Export\n",
    "# Never works smh...\n",
    "windows.to_csv('Data/ONEHOT_SCALED_WINDOWS.csv', index = None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
